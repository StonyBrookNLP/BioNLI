{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the code to prepare the data for entailment task. The input data is the training data for mechanism generation. The output is two sets of negative and positive sentences for the entailment task. The positive ones are the sentences themselves which are selected randomly. The negative ones are the actual sentences with a small twick to completely change the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2572988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "train_df=pd.read_csv('generation_out_train_v8/afterpretrain_all_4_no_entit_tag_v8/19/postprocess_out.csv')\n",
    "generation_df=pd.read_csv('data/v8_1/noEntity_inExp/split_data/combined_generation_neurologic_decoding.csv')\n",
    "generation_nd_SEN=pd.read_csv('neurologic_decoding/seq2seq/generation_negatives/SEN/generation_combined_SEN.csv')\n",
    "generation_nd_SRE=pd.read_csv('neurologic_decoding/seq2seq/generation_negatives/SRE/generation_combined_SRE.csv')\n",
    "combined_input=pd.read_csv('data/v8_1/noEntity_inExp/split_data/combined_input.csv')\n",
    "list(combined_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab76db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def negToPos(sent):\n",
    "    negative=False\n",
    "    new_sent=sent\n",
    "    if len(re.findall(r'\\bwas not\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bwas not\\b','was',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bwere not\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bwere not\\b','were',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bcannot\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bcannot\\b','can',sent,1)\n",
    "        negative= True\n",
    "    if len(re.findall(r'\\bis not\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bis not\\b','is',sent,1)\n",
    "        negative= True\n",
    "    if len(re.findall(r'\\bisn\\'t\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bisn\\'t\\b','is',sent,1)\n",
    "        negative= True\n",
    "    if len(re.findall(r'\\bwasn\\'t\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bwasn\\'t\\b','was',sent,1)\n",
    "        negative= True\n",
    "    if len(re.findall(r'\\baren\\'t\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\baren\\'t\\b','are',sent,1)\n",
    "        negative= True\n",
    "    if len(re.findall(r'\\bweren\\'t\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bweren\\'t\\b','were',sent,1)\n",
    "        negative= True\n",
    "    return new_sent,negative\n",
    "\n",
    "def posToNeg(sent):\n",
    "    # print(sent)\n",
    "    negative=False\n",
    "    new_sent=sent\n",
    "    if len(re.findall(r'\\bwas\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bwas\\b','was not',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bwere\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bwere\\b','were not',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bdo\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bdo\\b','do not',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    \n",
    "    if len(re.findall(r'\\bcan\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bcan\\b','cannot',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "\n",
    "\n",
    "\n",
    "    if len(re.findall(r'\\bis\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bis\\b','is not',sent,1)\n",
    "        negative= True\n",
    "    return new_sent,negative\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def swap_entity_tags(sent):\n",
    "    new_sent=sent.replace('<re>','<el1>').\\\n",
    "        replace('<er>','<le1>').\\\n",
    "            replace('<el>','<re>').\\\n",
    "                replace('<le>','<er>').\\\n",
    "                    replace('<el1>','<el>').\\\n",
    "                        replace('<le1>','<le>')\n",
    "    return new_sent,True\n",
    "\n",
    "def swap_entity_positions(sent):\n",
    "    try:\n",
    "        regulator=re.findall(\"<re>(.*?)<er>\",sent)[0].strip()\n",
    "        regulated=re.findall(\"<el>(.*?)<le>\",sent)[0].strip()\n",
    "        sent1=re.sub('<re> '+re.escape(regulator)+' <er>','<re1> '+regulated+ ' <er1>',sent)\n",
    "        sent2=re.sub('<el> '+re.escape(regulated)+' <le>','<re> '+regulator+ ' <er>',sent1)\n",
    "        sent3=re.sub('<re1> '+re.escape(regulated)+ ' <er1>','<el> '+regulated+' <le>',sent2)\n",
    "    except Exception as e:\n",
    "        print('error',e)\n",
    "        return(sent,False)\n",
    "    return(sent3,True)\n",
    "\n",
    "\n",
    "\n",
    "def swap_entity_names(sent):\n",
    "    regulator=re.findall(\"<re>(.*?)<er>\",sent)\n",
    "    regulated=re.findall(\"<el>(.*?)<le>\",sent)\n",
    "    if not regulator or not regulated:\n",
    "        return sent,False\n",
    "    regulator=regulator[0].strip()\n",
    "    regulated=regulated[0].strip()\n",
    "    sent1=re.sub('<re> '+re.escape(regulator)+' <er>','<re> '+regulated+ ' <er>',sent)\n",
    "    sent2=re.sub('<el> '+re.escape(regulated)+' <le>','<el> '+regulator+ ' <le>',sent1)\n",
    "    return sent2, True\n",
    "\n",
    "\n",
    "\n",
    "# def swap_random_entity(sent):\n",
    "#     regulator=re.findall(\"<re>(.*?)<er>\",sent)[0].strip()\n",
    "#     regulated=re.findall(\"<el>(.*?)<le>\",sent)[0].strip()\n",
    "#     nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "#     doc=nlp(sent)\n",
    "#     regulator_type=regulated_type=None\n",
    "#     entity_type_text_map={}\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.text.strip() == regulator.strip():\n",
    "#             regulator_type=ent.label_\n",
    "#             continue\n",
    "#         if ent.text.strip() == regulated.strip():\n",
    "#             regulated_type=ent.label_\n",
    "#             continue    \n",
    "#         if ent.label_ in entity_type_text_map:\n",
    "#             entity_type_text_map[ent.label_].append(ent.text)\n",
    "#         else:\n",
    "#             entity_type_text_map[ent.label_]=[ent.text]\n",
    "#     if not regulator_type and not regulated_type:\n",
    "#         return sent,False \n",
    "#     if regulator_type in entity_type_text_map:\n",
    "#         new_ent=random.choice(entity_type_text_map[regulator_type])\n",
    "#         new_sent=re.sub('<re> '+regulator+' <er>','<re> temppp <er>',sent)\n",
    "#         new_sent=re.sub(re.escape(new_ent),regulator,new_sent)\n",
    "#         new_sent=re.sub('<re> temppp <er>','<re> '+new_ent+' <er>',new_sent)\n",
    "#         return new_sent,True \n",
    "#     if regulated_type in entity_type_text_map:\n",
    "#         new_ent=random.choice(entity_type_text_map[regulated_type])\n",
    "#         new_sent=re.sub('<el> '+regulated+' <le>','<el> tempp <le>',sent)\n",
    "#         new_sent=re.sub(re.escape(new_ent),regulated,new_sent)\n",
    "#         new_sent=re.sub('<el> tempp <le>','<el> '+new_ent+ ' <le>',new_sent)\n",
    "#         return new_sent,True \n",
    "#     return sent,False\n",
    "\n",
    "def swap_random_entity_outside(sent,entity_type_map_reverse,entity_type_map):\n",
    "    regulator=re.findall(\"<re>(.*?)<er>\",sent)\n",
    "    regulated=re.findall(\"<el>(.*?)<le>\",sent)\n",
    "    if not regulator or not regulated:\n",
    "        return sent,False\n",
    "    regulator=regulator[0].strip()\n",
    "    regulated=regulated[0].strip()\n",
    "    regulator_type=regulated_type=None\n",
    "    if regulator.strip() in entity_type_map:\n",
    "            regulator_type=entity_type_map[regulator.strip()]\n",
    "    if regulated.strip() in entity_type_map:\n",
    "            regulated_type=entity_type_map[regulated.strip()]\n",
    "    if not regulator_type and not regulated_type:\n",
    "        return sent,False \n",
    "    if regulator_type:\n",
    "        possible_entities=entity_type_map_reverse[regulator_type]\n",
    "        new_ent=random.choice(possible_entities)\n",
    "        if new_ent.strip() != regulator.strip():\n",
    "            new_sent=re.sub('<re> '+re.escape(regulator)+' <er>','<re> temppp <er>',sent)\n",
    "            new_sent=re.sub(re.escape(new_ent),regulator,new_sent)\n",
    "            new_sent=re.sub('<re> temppp <er>','<re> '+new_ent+' <er>',new_sent)\n",
    "            return new_sent,True \n",
    "    if regulated_type:\n",
    "        new_ent=random.choice(entity_type_map_reverse[regulated_type])\n",
    "        if new_ent.strip() != regulated.strip():\n",
    "            new_sent=re.sub('<el> '+re.escape(regulated)+' <le>','<el> tempp <le>',sent)\n",
    "            new_sent=re.sub(re.escape(new_ent),regulated,new_sent)\n",
    "            new_sent=re.sub('<el> tempp <le>','<el> '+new_ent+ ' <le>',new_sent)\n",
    "            return new_sent,True \n",
    "    return sent,False\n",
    "\n",
    "## select a random entity from the supporting set and replace one of the main entities with an entity\n",
    "## with the same entity type\n",
    "def swap_random_entity_fromFile(sent,filePath='/home/mbastan/XP/entailment',index=0):\n",
    "    with open(os.path.join(filePath,'supp_set_entity_types.json')) as f:\n",
    "        lines=f.readlines()\n",
    "        supp_set_entity_type=json.loads(lines[index])\n",
    "    with open(os.path.join(filePath,'conclusion_entity_types.json')) as f:\n",
    "        lines=f.readlines()\n",
    "        concl_entity_type=json.loads(lines[index])\n",
    "    regulator=re.findall(\"<re>(.*?)<er>\",sent)\n",
    "    regulated=re.findall(\"<el>(.*?)<le>\",sent)\n",
    "    if not regulator or not regulated:\n",
    "        return sent,False\n",
    "    regulator=regulator[0].strip()\n",
    "    regulated=regulated[0].strip()\n",
    "    regulator_type=regulated_type=None\n",
    "    entity_type_text_map={}\n",
    "    if regulator.strip() in concl_entity_type:\n",
    "            regulator_type=concl_entity_type[regulator.strip()]\n",
    "    if regulated.strip() in concl_entity_type:\n",
    "            regulated_type=concl_entity_type[regulated.strip()]\n",
    "    if not regulator_type and not regulated_type:\n",
    "        return sent,False \n",
    "    for ent in supp_set_entity_type:\n",
    "        label=supp_set_entity_type[ent]\n",
    "        if label in entity_type_text_map:\n",
    "            entity_type_text_map[label].append(ent)\n",
    "        else:\n",
    "            entity_type_text_map[label]=[ent]\n",
    "\n",
    "    if regulator_type in entity_type_text_map:\n",
    "        new_ent=random.choice(entity_type_text_map[regulator_type])\n",
    "        if new_ent.strip() != regulator.strip():\n",
    "            new_sent=re.sub('<re> '+re.escape(regulator)+' <er>','<re> temppp <er>',sent)\n",
    "            new_sent=re.sub(re.escape(new_ent),regulator,new_sent)\n",
    "            new_sent=re.sub('<re> temppp <er>','<re> '+new_ent+' <er>',new_sent)\n",
    "            return new_sent,True \n",
    "    if regulated_type in entity_type_text_map:\n",
    "        new_ent=random.choice(entity_type_text_map[regulated_type])\n",
    "        if new_ent.strip() != regulated.strip():\n",
    "            new_sent=re.sub('<el> '+regulated+' <le>','<el> tempp <le>',sent)\n",
    "            new_sent=re.sub(re.escape(new_ent),regulated,new_sent)\n",
    "            new_sent=re.sub('<el> tempp <le>','<el> '+new_ent+ ' <le>',new_sent)\n",
    "            return new_sent,True \n",
    "    return sent,False\n",
    "\n",
    "## lexical polarity reverse: reverse the relational words with their antonyms\n",
    "def word_replace(sent):\n",
    "    negative = False\n",
    "    if len(re.findall(r'\\binhibits\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\binhibits\\b','promotes',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bpromotes\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bpromotes\\b','inhibits',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\binhibition\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\binhibition\\b','promotion',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\bpromotion\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bpromotion\\b','inhibition',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative\n",
    "    if len(re.findall(r'\\binhibitor\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\binhibitor\\b','promoter',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative  \n",
    "    if len(re.findall(r'\\bpromoter\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bpromoter\\b','inhibitor',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative \n",
    "\n",
    "    if len(re.findall(r'\\bincrease\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bincrease\\b','decrease',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative \n",
    "\n",
    "    if len(re.findall(r'\\bdecrease\\b',sent)) > 0:\n",
    "        new_sent = re.sub(r'\\bdecrease\\b','increase',sent,1)\n",
    "        negative= True\n",
    "        return new_sent,negative \n",
    "    return sent,False     \n",
    "\n",
    "# sswap one of the numbers frm the conclusion with another random number from the supporting set\n",
    "def swapNumber(line,supp_set):\n",
    "    line_nums=re.findall(r'(?:\\d*\\.\\d+|\\d+)', line)\n",
    "    abs_nums=re.findall(r'(?:\\d*\\.\\d+|\\d+)', supp_set)\n",
    "    if not line_nums or not abs_nums:\n",
    "        return line, False\n",
    "    ln_num=random.choice(line_nums)\n",
    "    ab_num=random.choice(abs_nums)\n",
    "    new_line=line\n",
    "    if ln_num!=ab_num or '%s)'%ln_num in line: ## if the number follows by ) it means its explaining something so it's not acceptable\n",
    "        # new_line=re.sub(r'\\b{}[^\\d.]\\b'.format(ln_num),ab_num,line)\n",
    "        # new_line=re.sub(r'\\b[-+]?(?:{})\\b'.format(ln_num),ab_num,line)\n",
    "        new_line=re.sub(r'(?<!\\.)\\b{}\\b(?!\\.)'.format(ln_num),ab_num,line)\n",
    "    if new_line==line:\n",
    "        return(new_line,False)\n",
    "    return new_line,True\n",
    "\n",
    "\n",
    "# select generated examples from the pretrained models select the ones that have bleurt score lower than 0.45 and\n",
    "# the relation between entities are predicted reverse from the ground truth\n",
    "def generatedSamples(line,gen_row,scr_threshold=0.45):\n",
    "    gen_exp = gen_row['BM_Exp']\n",
    "    gen_lbl=gen_row['BM_lbl']\n",
    "    gen_reg=gen_row['BM_reg']\n",
    "    gen_ele=gen_row['BM_ele']\n",
    "    true_lbl=gen_row['True_lbl']\n",
    "    tru_reg=gen_row['True_reg']\n",
    "    tru_ele=gen_row['True_ele']\n",
    "    bleu_scr=gen_row['BM_scr']\n",
    "    if gen_lbl == true_lbl:\n",
    "        return(line,False)\n",
    "    if pd.isna(gen_reg) or pd.isna(gen_ele):\n",
    "        return(line,False)\n",
    "    if gen_reg !=tru_reg or gen_ele!=tru_ele:\n",
    "        return(line,False)\n",
    "    if bleu_scr > scr_threshold:\n",
    "        return(line,False)\n",
    "    return(gen_exp,True)\n",
    "\n",
    "def generatedSamples_neurologicDecoding(line,gen_row):\n",
    "    gen_exp = gen_row['BM_Exp']\n",
    "    gen_lbl=gen_row['BM_lbl']\n",
    "    gen_reg=gen_row['BM_reg']\n",
    "    gen_ele=gen_row['BM_ele']\n",
    "    true_lbl=gen_row['True_lbl']\n",
    "    # if gen_lbl == true_lbl:\n",
    "    #     return(line,False)\n",
    "    if pd.isna(gen_reg) or pd.isna(gen_ele) or pd.isna(gen_exp):\n",
    "        return(line,False)\n",
    "    regulators=re.findall(\"<re>(.*?)<er>\",line)\n",
    "    regulateds=re.findall(\"<el>(.*?)<le>\",line)\n",
    "\n",
    "    regulators_g=re.findall(\"<re>(.*?)<er>\",gen_exp)\n",
    "    regulateds_g=re.findall(\"<el>(.*?)<le>\",gen_exp)\n",
    "    if len(set(regulators_g)) >1 or len(set(regulateds_g)) >1: ## if multiple entities marked as regulator or regulatee, we don;t like it\n",
    "        return(line,False)\n",
    "\n",
    "    regulator=regulators[0].strip().lower()\n",
    "    regulated=regulateds[0].strip().lower()\n",
    "    regulated_g=regulateds_g[0].strip().lower()\n",
    "    regulator_g=regulators_g[0].strip().lower()\n",
    "\n",
    "    if regulator != regulated_g or regulated != regulator_g: ## if the entities are not reversed this is not what we want\n",
    "        return(line,False)\n",
    "\n",
    "    return(gen_exp,True)\n",
    "\n",
    "\n",
    "def generatedSamples_nd_SEN(line,gen_row):\n",
    "    \n",
    "    try:\n",
    "        gen_exp = gen_row['generated_nd'].iloc[0][6:-16] ## skip first <exp> token and last polarity label\n",
    "        if len(set(re.findall(\"<el>(.*?)<le>\",gen_exp))) >1 : ## multiple entities marked as element\n",
    "            return(line, False)\n",
    "        if len(set(re.findall(\"<re>(.*?)<er>\",gen_exp))) >1 : ## multiple entities marked as regulator\n",
    "            return(line, False)\n",
    "        \n",
    "        if gen_row['sat_er_and_ed'].iloc[0] < 1:\n",
    "            return(line,False)\n",
    "        return(gen_exp,True)\n",
    "    except Exception as e:\n",
    "        print('error ocurred:',e)\n",
    "        return(line,False)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "model = SentenceTransformer('michiyasunaga/BioLinkBERT-base')#'bert-base-nli-mean-tokens')\n",
    "threhsold=0.94\n",
    "\n",
    "def generatedSamples_nd_SRE(line,gen_row):\n",
    "    try:\n",
    "        gen_exp = gen_row['generated_nd'].iloc[0][6:-16]\n",
    "\n",
    "        if len(set(re.findall(\"<el>(.*?)<le>\",gen_exp))) >1 : ## multiple entities marked as element\n",
    "            return(line, False)\n",
    "        if len(set(re.findall(\"<re>(.*?)<er>\",gen_exp))) >1 : ## multiple entities marked as regulator\n",
    "            return(line, False)\n",
    "        if gen_row['sat_er_and_ed'].iloc[0] < 1:\n",
    "            return(line,False)\n",
    "        sentence_embeddings = model.encode([gen_exp,line])\n",
    "        if cosine_similarity([sentence_embeddings[1]],[sentence_embeddings[0]]) > 0.90: ## if two sentences are too similar skip this\n",
    "                return (line,False)\n",
    "        return(gen_exp,True)\n",
    "    except:\n",
    "        return(line,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcddf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_map_reverse={}\n",
    "entity_type_map={}\n",
    "with open('entailment/supp_set_entity_types_v2.json') as f:\n",
    "    lines=f.readlines()\n",
    "for line in lines:\n",
    "    supp_set_entity_type=json.loads(line)\n",
    "    for item in supp_set_entity_type:\n",
    "        if not item in entity_type_map:\n",
    "            entity_type_map[item]=supp_set_entity_type[item]\n",
    "        typee=supp_set_entity_type[item]\n",
    "        if typee in entity_type_map_reverse:\n",
    "            entity_type_map_reverse[typee].append(item)\n",
    "        else:\n",
    "            entity_type_map_reverse[typee]=[item]\n",
    "print(len(entity_type_map_reverse))\n",
    "print(len(entity_type_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14843af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative=0):\n",
    "    ## remove extra space\n",
    "    new_line=re.sub(' +', ' ', new_line)\n",
    "    row = {'supp_set':supp_set.strip(),\\\n",
    "                'conclusion':new_line.strip(),\\\n",
    "                'abstract':supp_set.strip()+ ' <exp> '+new_line.strip(),\\\n",
    "                'label_cat':labels_cat,\n",
    "                'ori_conclusion':line.strip(),\n",
    "                'pmid':pmid,\n",
    "                'index':index}\n",
    "                \n",
    "    return(data_df.append(row,ignore_index=True),negative - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f80715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "LPR_count=0 # word replace or lexical polarity reversal\n",
    "negToPos_count=0\n",
    "posToNeg_count=0\n",
    "swap_number_count=0\n",
    "generation_count=0\n",
    "generation_nd_count=0\n",
    "generation_nd_SRE_count=0\n",
    "generation_nd_SEN_count=0\n",
    "neg_constraint=0\n",
    "SRE_count=0 #swap_random_entity\n",
    "SEP_count=0 #swap_entity_positions\n",
    "SET_count=0 #swap_entity_tags\n",
    "SEN_count=0 #swap_entity_names\n",
    "SREO_count=0 # swap with a random entity from outside the context\n",
    "negative_sents = []\n",
    "positives =0\n",
    "positive_sents =[]\n",
    "negative_ids =[]\n",
    "positive_ids = []\n",
    "negative_supp_set=[]\n",
    "positive_supp_set=[]\n",
    "labels= []\n",
    "data_df=pd.DataFrame([])\n",
    "\n",
    "\n",
    "\n",
    "# train_df.sample(frac=1)\n",
    "# train_exp=list(train_df['True_Exp'])\n",
    "# train_supp=list(train_df['Sup_Sen'])\n",
    "\n",
    "\n",
    "combined_input.sample(frac=1)\n",
    "train_exp=list(combined_input['conclusion'])\n",
    "train_supp=list(combined_input['original_supp'])\n",
    "pmids=list(combined_input['pmid'])\n",
    "\n",
    "entity_type_map_reverse={}\n",
    "entity_type_map={}\n",
    "with open('entailment/supp_set_entity_types.json') as f:\n",
    "    lines=f.readlines()\n",
    "for line in lines:\n",
    "    supp_set_entity_type=json.loads(line)\n",
    "    for item in supp_set_entity_type:\n",
    "        if not item in entity_type_map:\n",
    "            entity_type_map[item]=supp_set_entity_type[item]\n",
    "        typee=supp_set_entity_type[item]\n",
    "        if typee in entity_type_map_reverse:\n",
    "            entity_type_map_reverse[typee].append(item)\n",
    "        else:\n",
    "            entity_type_map_reverse[typee]=[item]\n",
    "\n",
    "## entailment data versioning:\n",
    "base_path=\"entailment_data/v10\"\n",
    "index=-1\n",
    "null_values=0\n",
    "for line,supp_set,pmid in zip(train_exp,train_supp,pmids):\n",
    "    index+=1\n",
    "    if index % 1000 ==0:\n",
    "        print('processed %d items'%index)\n",
    "    # break\n",
    "    if pd.isna(line) or pd.isna(supp_set) or not re.search(\"<re>(.*?)<er>\",line) or not re.search(\"<el>(.*?)<le>\",line):\n",
    "        null_values+=1\n",
    "        continue\n",
    "\n",
    "\n",
    "    if True: \n",
    "        positive_sents.append(line)\n",
    "        new_line = line\n",
    "        positive_supp_set.append(supp_set)\n",
    "        positives += 1\n",
    "        positive_ids.append(pmid)\n",
    "        labels_cat = 'pos'\n",
    "        data_df,_=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid)\n",
    "    if True: \n",
    "        negative_supp_set.append(supp_set)\n",
    "        negative_ids.append(pmid)\n",
    "        negative_c =False\n",
    "        negative=5\n",
    "        if True: \n",
    "            c_df=generation_nd_SEN[generation_nd_SEN['pmid']==float(pmid)]\n",
    "            if len(c_df) > 0:\n",
    "                new_line, negative_c=generatedSamples_nd_SEN(line,c_df)\n",
    "\n",
    "            if negative_c:\n",
    "                labels_cat='generation_nd_SEN'\n",
    "                negative_sents.append(new_line)\n",
    "                generation_nd_SEN_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "        if True:\n",
    "            c_df=generation_nd_SRE[generation_nd_SRE['pmid']==float(pmid)]\n",
    "            if len(c_df) > 0:\n",
    "                new_line, negative_c=generatedSamples_nd_SRE(line,c_df)\n",
    "\n",
    "            if negative_c:\n",
    "                labels_cat='generation_nd_SRE'\n",
    "                negative_sents.append(new_line)\n",
    "                generation_nd_SRE_count += 1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "                # repetition -=1\n",
    "\n",
    "\n",
    "        if True:\n",
    "            c_df=generation_df[generation_df['Sup_Sen']==supp_set]\n",
    "            if len(c_df) > 0:\n",
    "                new_line, negative_c=generatedSamples(line,c_df.iloc[0]) \n",
    "\n",
    "\n",
    "            if negative_c:\n",
    "                labels_cat='generation'\n",
    "                negative_sents.append(new_line)\n",
    "                generation_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "                # repetition -=1\n",
    "\n",
    "        if True:#if  repetition>0 and negative>0 and random.random()<1:\n",
    "            try:\n",
    "                new_line,negative_c=swap_random_entity_fromFile(line,index=index)\n",
    "            except Exception as e:\n",
    "                # print('error darim',str(e))\n",
    "                pass\n",
    "            if negative_c:\n",
    "                labels_cat='SRE' # swap random entity\n",
    "                negative_sents.append(new_line)\n",
    "                SRE_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=swap_entity_positions(line)\n",
    "            if negative_c:\n",
    "                labels_cat='SEP' # swap entity positions\n",
    "                negative_sents.append(new_line)\n",
    "                SEP_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=swap_entity_tags(line)\n",
    "            if negative_c:\n",
    "                labels_cat='SET' # swap entity tags\n",
    "                negative_sents.append(new_line)\n",
    "                SET_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "        if True:\n",
    "            c_df=generation_df[generation_df['pmid']==float(pmid)]\n",
    "            if len(c_df) > 0:\n",
    "                new_line,negative_c=generatedSamples_neurologicDecoding(line,c_df.iloc[0])\n",
    "                if negative_c:\n",
    "                    labels_cat='generation_nd' # swap entity names\n",
    "                    negative_sents.append(new_line)\n",
    "                    generation_nd_count +=1\n",
    "                    data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "                    \n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=swap_random_entity_outside(line,entity_type_map_reverse,entity_type_map)\n",
    "            if negative_c:\n",
    "                labels_cat='SREO' # swap random entity from outside the txt\n",
    "                negative_sents.append(new_line)\n",
    "                SREO_count +=1\n",
    "                data_df, negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line, negative_c=swapNumber(line,supp_set)\n",
    "\n",
    "            if negative_c:\n",
    "                labels_cat='swap_number'\n",
    "                negative_sents.append(new_line)\n",
    "                swap_number_count +=1\n",
    "                data_df, negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=word_replace(line)\n",
    "            if negative_c:\n",
    "                labels_cat='LPR'\n",
    "                negative_sents.append(new_line)\n",
    "                LPR_count += 1 \n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=posToNeg(line)\n",
    "            if negative_c:\n",
    "                labels_cat='posToNeg'\n",
    "                negative_sents.append(new_line)\n",
    "                posToNeg_count += 1 \n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "        if True:\n",
    "                new_line,negative_c=negToPos(line)\n",
    "                if negative_c:\n",
    "                    labels_cat='negToPos'\n",
    "                    negative_sents.append(new_line)\n",
    "                    negToPos_count += 1\n",
    "                    data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "\n",
    "\n",
    "        if True:\n",
    "            new_line,negative_c=swap_entity_names(line)\n",
    "            if negative_c:\n",
    "                labels_cat='SEN' # swap entity names\n",
    "                negative_sents.append(new_line)\n",
    "                SEN_count +=1\n",
    "                data_df,negative=add_row(supp_set,new_line,labels_cat,line,data_df,index,pmid,negative)\n",
    "\n",
    "    if index % 1000 ==0:\n",
    "        data_df.to_csv(os.path.join(base_path,'entailment_data.csv'),index=False)\n",
    "        with open(os.path.join(base_path,'positive_conclusion.txt'),'w') as f:\n",
    "            for item in positive_sents:\n",
    "                f.write(item)\n",
    "                f.write('\\n')\n",
    "        with open(os.path.join(base_path,'negative_conclusion.txt'),'w') as f:\n",
    "            for item in negative_sents:\n",
    "                f.write(item)\n",
    "                f.write('\\n')\n",
    "\n",
    "        with open(os.path.join(base_path,'positive_suppset.txt'),'w') as f:\n",
    "            for item in positive_supp_set:\n",
    "                f.write(item)\n",
    "                f.write('\\n')\n",
    "        with open(os.path.join(base_path,'negative_suppset.txt'),'w') as f:\n",
    "            for item in negative_supp_set:\n",
    "                f.write(item)\n",
    "                f.write('\\n')\n",
    "\n",
    "        with open(os.path.join(base_path,'negative_ids.pk'),'wb') as f:\n",
    "            pickle.dump(negative_ids,f)\n",
    "        with open(os.path.join(base_path,'positive_ids.pk'),'wb') as f:\n",
    "            pickle.dump(positive_ids,f)\n",
    "\n",
    "        with open(os.path.join(base_path,'labels_cat.pk'),'wb') as f:\n",
    "            pickle.dump(labels,f)\n",
    "\n",
    "print('number of null values:', null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eee584",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('positives: %d, LPR_count: %d, generation_count: %d ,generation_nd_count: %d, generation_nd_SRE_count: %d, generation_nd_SEN_count: %d ,SET_count: %d ,SEN_count: %d \\\n",
    ",SEP_count: %d ,SRE_count: %d ,SREO_count: %d , swap_number_count: %d, posToNeg: %d, negToPos: %d'%\\\n",
    "        (positives, LPR_count, generation_count,generation_nd_count, generation_nd_SRE_count, generation_nd_SEN_count, SET_count,SEN_count,\\\n",
    "            SEP_count,SRE_count, SREO_count, swap_number_count, posToNeg_count, negToPos_count))\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd17875",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(os.path.join(base_path,'entailment_data.csv'),index=False)\n",
    "with open(os.path.join(base_path,'positive_conclusion.txt'),'w') as f:\n",
    "    for item in positive_sents:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "with open(os.path.join(base_path,'negative_conclusion.txt'),'w') as f:\n",
    "    for item in negative_sents:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(os.path.join(base_path,'positive_suppset.txt'),'w') as f:\n",
    "    for item in positive_supp_set:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "with open(os.path.join(base_path,'negative_suppset.txt'),'w') as f:\n",
    "    for item in negative_supp_set:\n",
    "        f.write(item)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(os.path.join(base_path,'negative_ids.pk'),'wb') as f:\n",
    "    pickle.dump(negative_ids,f)\n",
    "with open(os.path.join(base_path,'positive_ids.pk'),'wb') as f:\n",
    "    pickle.dump(positive_ids,f)\n",
    "\n",
    "with open(os.path.join(base_path,'labels_cat.pk'),'wb') as f:\n",
    "    pickle.dump(labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "percentage = lambda i: len(i) / float(len(count_list)) * 100\n",
    "\n",
    "per_list=[]\n",
    "for item in set(count_list):\n",
    "    per_list.append(count_list.count(item)/float(len(count_list)) * 100)\n",
    "\n",
    "ax = sns.barplot(x=count_list, y=count_list,  estimator=percentage, color='b',edgecolor='black')\n",
    "\n",
    "ax.set(ylabel=\"Percent\")\n",
    "ax.set(xlabel=\"Number of Possible Perturbations\")\n",
    "ax.set(title='Possible Perturbations in Dev Set')\n",
    "patches = ax.patches\n",
    "for i in range(len(patches)):\n",
    "   x = patches[i].get_x() + patches[i].get_width()/2\n",
    "   y = patches[i].get_height()+.05\n",
    "   ax.annotate('{:.1f}%'.format(per_list[i]), (x, y), ha='center')\n",
    "fig=ax.get_figure()\n",
    "fig.savefig('data_dist.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
